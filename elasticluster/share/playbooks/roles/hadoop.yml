---


- name: Install Hadoop master node
  hosts: hadoop_master
  tags:
    - hadoop
    - master
  vars:
    # PySpark won't currently work with Python 3.6 (see:
    # https://issues.apache.org/jira/browse/SPARK-19019) so force an older
    # version of Anaconda Python to be installed: with Python 3.5 all looks OK
    # fixed in Spark > 2.1.1
    anaconda_version: '4.2.0'
  roles:
    - role: 'nis'
      NIS_MASTER: '{{groups.hadoop_master[0]}}'
    - hdfs-namenode
    - role: hive-server
      HIVE_METASTORE_HOST: '{{groups.hadoop_master[0]}}'
      HIVE_CLIENTS: '{{groups.hadoop_master + groups.hadoop_worker}}'
    - yarn-master
    - spark-common
    - spark-history
    - {role: jupyterhub, when: jupyter is defined}
    - {role: rstudio, when: rserver is defined}


- name: Install Hadoop worker nodes
  hosts: hadoop_worker
  tags:
    - hadoop
    - worker
  vars:
    # Python versions should never differ between master and workers, otherwise pyspark won't work
    anaconda_version: '4.2.0'
  roles:
    - role: 'nis'
      NIS_MASTER: '{{groups.hadoop_master[0]}}'
    - hdfs-datanode
    - role: hive
      HIVE_METASTORE_HOST: '{{groups.hadoop_master[0]}}'
    - yarn-worker
    - spark-common

