---

- name: Install Hadoop master node
  hosts: hadoop_master
  tags:
    - hadoop
    - master
  vars:
    # PySpark won't currently work with Python 3.6 (see:
    # https://issues.apache.org/jira/browse/SPARK-19019) so force an older
    # version of Anaconda Python to be installed: with Python 3.5 all looks OK
    anaconda_version: '4.2.0'
  roles:
    - role: 'nis'
      NIS_MASTER: '{{groups.hadoop_master[0]}}'
      when: '{{multiuser_cluster|default("true")|bool}}'
    - hdfs-namenode
    - role: hive-server
      HIVE_METASTORE_HOST: '{{groups.hadoop_master[0]}}'
      HIVE_CLIENTS: '{{groups.hadoop_master + groups.hadoop_worker}}'
    - yarn-master
    - spark-master


- name: Install Hadoop worker nodes
  hosts: hadoop_worker
  tags:
    - hadoop
    - worker
  roles:
    - role: 'nis'
      NIS_MASTER: '{{groups.hadoop_master[0]}}'
      when: '{{multiuser_cluster|default("true")|bool}}'
    - hdfs-datanode
    - role: hive
      HIVE_METASTORE_HOST: '{{groups.hadoop_master[0]}}'
    - yarn-worker
    - spark-worker

