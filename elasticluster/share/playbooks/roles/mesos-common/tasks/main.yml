---

- name: Fail if not systemd
  fail:
    msg: "systemd is required for this mesos playbook"
  when: not init_is_systemd

- name: Add apt-key (Debian/Ubuntu)
  apt_key: id=E56151BF keyserver=keyserver.ubuntu.com state=present
  when: is_debian_or_ubuntu

- name: Add mesosphere repo (Debian/Ubuntu)
  apt_repository:
    repo='deb http://repos.mesosphere.com/{{ ansible_distribution | lower }} {{ ansible_distribution_release | lower }} main'
    state=present
    update_cache=yes
  when: is_debian_or_ubuntu

- name: Add mesosphere repository (Redhat/Centos)
  yum: name=https://repos.mesosphere.com/el/{{ ansible_distribution_major_version | int }}/noarch/RPMS/mesosphere-el-repo-{{ ansible_distribution_major_version | int }}-1.noarch.rpm state=present
  when: is_rhel_compatible

- name: register python dev (Debian/Ubuntu)
  set_fact:
    python_dev_pack: python-dev
  when: is_debian_or_ubuntu

- name: register python dev (RHEL/Centos)
  set_fact:
    python_dev_pack: python-devel
  when: is_rhel_compatible

- name: Install Mesosphere on master nodes
  package:
    name: mesosphere
    state: present
  when: is_debian_or_ubuntu

- name: Install Mesos and dependencies
  package: name={{ item }} state=present
  with_items:
    - wget
    - curl
    - unzip
    - python-setuptools
    - "{{python_dev_pack}}"
    - mesos
    - mesosphere-zookeeper

- name: Mesos ZK config file
  template: src=zk.j2 dest=/etc/mesos/zk

- name: Create Apache Spark group
  group:
    name: spark
    state: present

- name: Create Apache Spark user
  user:
    name: spark
    group: spark

- name: Download Spark
  get_url:
    url: '{{mesos_spark_uri}}'
    dest: '/tmp/{{mesos_spark_package}}'

- name: Unpack Spark
  unarchive:
    src: '/tmp/{{mesos_spark_package}}'
    dest: /usr/lib/
    owner: spark
    group: spark
    remote_src: True

- name: Create Spark symlink
  file: src="/usr/lib/{{mesos_spark_package_root}}" dest="/usr/lib/spark" state=link

- name: Check if Py4J version-independent `.zip` archive exist
  stat:
    path='/usr/lib/spark/python/lib/py4j-src.zip'
    follow=no
  register: p

- name: Symlink Py4J `.zip` archive under a version-independent name
  # XXX: We use shell globbing to find out what source file to link,
  # assuming there is one and only one Py4J `.zip` file.
  shell: |
    cd /usr/lib/spark/python/lib
    ln -s py4j-*-src.zip py4j-src.zip
  when: not p.stat.exists

- name: Spark configuration files
  template:
    src='{{item}}.j2'
    dest='/usr/lib/spark/conf/{{item}}'
    owner=spark
    group=spark
  with_items:
    - spark-defaults.conf
    - spark-env.sh

- name: Deploy Spark profile configuration files
  copy:
    src='{{item}}'
    dest='/{{item}}'
  with_items:
    - etc/profile.d/spark.sh
    - etc/profile.d/pyspark.sh

- name: Create Spark log directory
  file:
    path='/var/log/spark'
    state=directory
    owner=spark
    group=spark

- name: Set SPARK_HOME in environment
  lineinfile: dest=/etc/environment line='export SPARK_HOME=/usr/lib/spark' insertafter='EOF' state=present
