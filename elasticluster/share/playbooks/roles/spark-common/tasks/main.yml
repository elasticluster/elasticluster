---

- name: Fail if not Ubuntu 16 or greater
  fail:
    msg: "Ubuntu is 16 or greater is required for mesos playbook"
  when: not( ansible_distribution == 'Ubuntu' and ansible_distribution_major_version|int >= 16)

- name: Fail if more than 1 master
  fail:
    msg: "Only single master supported when running Spark, for HA use Hadoop or Mesos"
  when: not(groups.spark_master|length == 1)

- stat:
    path: /usr/lib/alluxio
  register: alluxio_symlink

- name: Create Apache Alluxio group
  group:
    name: alluxio
    state: present
  when: alluxio_symlink.stat.islnk is not defined

- name: Create Apache Alluxio user
  user:
    name: alluxio
    group: alluxio
    system: True
  when: alluxio_symlink.stat.islnk is not defined

- name: Add limited sudo rights for alluxio user
  lineinfile:
    dest: /etc/sudoers
    state: present
    regexp: '^alluxio'
    line: 'alluxio ALL=(ALL) NOPASSWD: /bin/mount * /mnt/ramdisk, /bin/umount * /mnt/ramdisk, /bin/mkdir * /mnt/ramdisk, /bin/chmod * /mnt/ramdisk'
  when: alluxio_symlink.stat.islnk is not defined

- name: Download Alluxio
  get_url:
    url: '{{spark_alluxio_uri}}'
    dest: '/tmp/{{spark_alluxio_package}}'
  when: alluxio_symlink.stat.islnk is not defined

- name: Unpack Alluxio
  unarchive:
    src: '/tmp/{{spark_alluxio_package}}'
    dest: /usr/lib/
    owner: alluxio
    group: alluxio
    remote_src: True
  when: alluxio_symlink.stat.islnk is not defined

- name: Create Alluxio symlink
  file: src="/usr/lib/{{spark_alluxio_package_root}}" dest="/usr/lib/alluxio" state=link
  when: alluxio_symlink.stat.islnk is not defined

- name: Create Alluxio log directory
  file:
    path='/var/log/alluxio'
    state=directory
    owner=alluxio
    group=alluxio
  when: alluxio_symlink.stat.islnk is not defined

- name: Create Alluxio RAM directory
  file:
    path='/mnt/ramdisk'
    state=directory
    owner=alluxio
    group=alluxio
  when: alluxio_symlink.stat.islnk is not defined

- name: Create Alluxio underFS directory
  file:
    path='/usr/lib/alluxio/underFSStorage'
    state=directory
    owner=alluxio
    group=alluxio
  when: alluxio_symlink.stat.islnk is not defined

- name: Create Alluxio Configuration symlink
  file: src="/usr/lib/alluxio/conf" dest="/etc/alluxio" state=link
  when: alluxio_symlink.stat.islnk is not defined

- name: Alluxio configuration files
  template:
    src='{{item}}.j2'
    dest='/usr/lib/alluxio/conf/{{item}}'
    owner=alluxio
    group=alluxio
  with_items:
    - alluxio-site.properties
    - alluxio-env.sh
    - masters
    - workers
  when: alluxio_symlink.stat.islnk is not defined

- name: Set ALLUXIO_HOME in environment
  lineinfile: dest=/etc/environment line='export ALLUXIO_HOME=/usr/lib/alluxio' insertafter='EOF' state=present
  when: alluxio_symlink.stat.islnk is not defined

- stat:
    path: /usr/lib/spark
  register: spark_symlink

- name: Create Apache Spark group
  group:
    name: spark
    state: present
  when: spark_symlink.stat.islnk is not defined

- name: Create Apache Spark user
  user:
    name: spark
    group: spark
    system: True
    createhome: False
  when: spark_symlink.stat.islnk is not defined

- name: Download Spark
  get_url:
    url: '{{spark_uri}}'
    dest: '/tmp/{{spark_package}}'
  when: spark_symlink.stat.islnk is not defined

- name: Unpack Spark
  unarchive:
    src: '/tmp/{{spark_package}}'
    dest: /usr/lib/
    owner: spark
    group: spark
    remote_src: True
  when: spark_symlink.stat.islnk is not defined

- name: Create Spark symlink
  file: src="/usr/lib/{{spark_package_root}}" dest="/usr/lib/spark" state=link
  when: spark_symlink.stat.islnk is not defined

- name: Check if Py4J version-independent `.zip` archive exist
  stat:
    path='/usr/lib/spark/python/lib/py4j-src.zip'
    follow=no
  register: p
  when: spark_symlink.stat.islnk is not defined

- name: Symlink Py4J `.zip` archive under a version-independent name
  shell: |
    cd /usr/lib/spark/python/lib
    ln -s py4j-*-src.zip py4j-src.zip
  when: spark_symlink.stat.islnk is not defined and not p.stat.exists

- name: Create Spark Configuration symlink
  file: src="/usr/lib/spark/conf" dest="/etc/spark" state=link
  when: spark_symlink.stat.islnk is not defined

- name: Spark configuration files
  template:
    src='{{item}}.j2'
    dest='/usr/lib/spark/conf/{{item}}'
    owner=spark
    group=spark
  with_items:
    - spark-defaults.conf
    - spark-env.sh
  when: spark_symlink.stat.islnk is not defined

- name: Create Spark log directory
  file:
    path='/var/log/spark'
    state=directory
    owner=spark
    group=spark
  when: spark_symlink.stat.islnk is not defined

- name: Set SPARK_HOME in environment
  lineinfile: dest=/etc/environment line='export SPARK_HOME=/usr/lib/spark' insertafter='EOF' state=present
  when: spark_symlink.stat.islnk is not defined

- name: Deploy profile configuration files
  copy:
    src='{{item}}'
    dest='/{{item}}'
  with_items:
    - etc/profile.d/spark.sh
    - etc/profile.d/alluxio.sh
  when: alluxio_symlink.stat.islnk is not defined or spark_symlink.stat.islnk is not defined

- name: Override default Spark Shell behaviour (disable hive)
  copy:
    src='usr/lib/spark/python/pyspark/shell.py'
    dest='/usr/lib/spark/python/pyspark/shell.py'
    owner=spark
    group=spark
  when: spark_symlink.stat.islnk is not defined

- name: Install Auxiliary Python Packages using pip
  pip:
    name='{{item}}'
    state=present
    executable='{{spark_jupyter_python|dirname}}/pip'
  with_items: '{{spark_anaconda_aux_packages}}'
  register: pip_install
  ignore_errors: yes

- name: Install Auxiliary Python Packages using conda
  conda:
    name='{{item}}'
    state=present
    executable='{{spark_jupyter_python|dirname}}/conda'
  with_items: '{{spark_anaconda_aux_packages}}'
  register: conda_install
  ignore_errors: yes
  when: pip_install|failed

- name: Fail if packages could not be installed with pip or conda
  fail:
    msg: 'could not install all packages, please check if they are avaible for Python {{anaconda_python_version}} (pip or conda)'
  when: conda_install|failed
