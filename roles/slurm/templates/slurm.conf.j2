# THIS FILE IS CONTROLLED BY CFENGINE,
# any local modifications will be overwritten!
# If you want to make changes, please edit the
# master file in: 
#   ocikbgs.uzh.ch:masterfiles/hosts/SLURM.d/etc/slurm-llnl/slurm.conf

# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
# 
ControlMachine={{ groups['slurm_master'][0] }}
ControlAddr={{ hostvars[groups['slurm_master'][0]].ansible_default_ipv4.address }}
#BackupController=slurm2
#BackupAddr=192.168.160.6
# 
AuthType=auth/munge
CacheGroups=0
CheckpointType=checkpoint/none 
CryptoType=crypto/munge
#
DisableRootJobs=NO 
#EnforcePartLimits= 
#Epilog=
#PrologSlurmctld=
# 
FirstJobId=1 
MaxJobId=4294901760
#GresTypes= 
#GroupUpdateForce=0
# 
#GroupUpdateTime=600 
JobCheckpointDir=/var/lib/slurm-llnl/checkpoint 
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#
#JobFileAppend=0 
JobRequeue=1 
#JobSubmitPlugins=1 
#KillOnBadExit=0 
#
#Licenses=foo*4,bar 
#MailProg=/usr/bin/mail 
#MaxJobCount=5000 
#MaxStepCount=40000 
#
MaxTasksPerNode=8 
MpiDefault=openmpi
# Note: Apparently, the `MpiParams` option is needed also for non-mpi
# jobs in slurm 2.5.3.
MpiParams=ports=12000-12999
#PluginDir= 
#
#PlugStackConfig= 
#PrivateData=jobs 
ProctrackType=proctrack/linuxproc
#Prolog=
#
#PrologSlurmctld= 
#PropagatePrioProcess=0 
#PropagateResourceLimits= 
#PropagateResourceLimitsExcept= 
#
ReturnToService=2
#SallocDefaultCommand= 
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
SlurmctldPort=6817
#
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser=slurm
#
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
#
# TaskEpilog=/site/apps/slurm/bin/task_epilog
# TaskPlugin=task/none
#TaskPluginParam=
# TaskProlog=/site/apps/slurm/bin/task_prolog
#
#TopologyPlugin=topology/tree 
TmpFs=/var/tmp 
#TrackWCKey=no 
#TreeWidth= 
#
#UnkillableStepProgram= 
#UsePAM=0 
# 
# 
# TIMERS 
#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
#HealthCheckInterval=0 
#HealthCheckProgram= 
InactiveLimit=0
KillWait=30
#MessageTimeout=10 
#ResvOverRun=0 
MinJobAge=300
#OverTimeLimit=0 
SlurmctldTimeout=30
SlurmdTimeout=300
#UnkillableStepTimeout=60 
#VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
#DefMemPerCPU=0 
FastSchedule=0
#MaxMemPerCPU=0 
#SchedulerRootFilter=1 
#SchedulerTimeSlice=30 
SchedulerType=sched/backfill
SchedulerPort=7321
# NOTE: If we don't know the number of CPUs of each node, we need to
# set FastSchedule=0 and SelectType=select/linear Please check SLURM's
# FAQ and manpage of slurm.conf
# https://computing.llnl.gov/linux/slurm/faq.html#fast_schedule
SelectType=select/linear
SelectTypeParameters=CR_Memory
# 
# 
# JOB PRIORITY 
#PriorityType=priority/basic 
#PriorityDecayHalfLife= 
#PriorityCalcPeriod= 
#PriorityFavorSmall= 
#PriorityMaxAge= 
#PriorityUsageResetPeriod= 
#PriorityWeightAge= 
#PriorityWeightFairshare= 
#PriorityWeightJobSize= 
#PriorityWeightPartition= 
#PriorityWeightQOS= 
# 
# 
# LOGGING AND ACCOUNTING 
#AccountingStorageEnforce=0 
#AccountingStorageHost=
AccountingStorageLoc=/var/tmp/slurm-acct.txt
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/filetxt
#AccountingStorageUser=
AccountingStoreJobComment=YES
ClusterName=gc3cluster
#DebugFlags= 
#JobCompHost=
JobCompLoc=/var/tmp/slurm-jobs.txt
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/filetxt
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
#SuspendProgram= 
#ResumeProgram= 
#SuspendTimeout= 
#ResumeTimeout= 
#ResumeRate= 
#SuspendExcNodes= 
#SuspendExcParts= 
#SuspendRate= 
#SuspendTime= 
# 
# 
# Sockets=1 CoresPerSocket=1 ThreadsPerCore=1
{% for host in groups['slurm_clients'] %}
NodeName={{host}} 
{% endfor %}
PartitionName=cloud Nodes={% for host in groups['slurm_clients'] %}{{host}},{%endfor%} MaxTime=INFINITE State=UP Default=YES

