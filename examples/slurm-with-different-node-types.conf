#
# Example: deploy a SLURM cluster with compute nodes of 2 different sizes
#
# This example covers the installation of a SLURM cluster with 4
# compute nodes, which fall in two different categories: "small" and
# "large", and one login/front-end node. It is a small variation on
# the configuration presented in file `slurm.conf` in this directory.
#
# There is nothing special about the two node types here: there could
# be more node types, or the differences could be in the installed
# software (e.g., different base OS, or nodes of a certain kind have a
# GPU and hence install CUDA).  The important point is that
# ElastiCluster does not attach any special significance to node class
# names: they are just labels to identify a set of software and
# configurations to be applied.
#
# This configuration file is *incomplete* and needs to be complemented
# with a suitable `cloud` section.
#
# For more details about the configuration, see:
# - http://elasticluster.readthedocs.io/en/latest/configure.html
# - http://elasticluster.readthedocs.io/en/latest/playbooks.html#id4
#

[cluster/slurm-on-ubuntu16]
# note that we need to reference a `setup` section where the three
# classes `master`, `small_worker`, `large_worker` are defined; if
# not, the configuration is invalid and ElastiCluster rejects it with
# an error
setup=slurm2
master_nodes=1
small_worker_nodes=2
large_worker_nodes=2
ssh_to=master

# this is cloud-specific info (using OpenStack for the example)
cloud=openstack
flavor=1cpu-4ram-hpc
network_ids=c86b320c-9542-4032-a951-c8a068894cc2
security_group=default

# Ubuntu 16.04.1 + python2.7
image_id=bab386b3-2c21-4a67-a146-a658668ac096

# `login` info is -in theory- image-specific
login=ubuntu


# now give worker nodes a different flavor
[cluster/slurm-on-ubuntu16/small_worker]
flavor=4cpu-16ram-hpc

[cluster/slurm-on-ubuntu16/large_worker]
flavor=32cpu-128ram-hpc


# this `slurm2` section is similar to the `[setup/slurm]` of other
# example configuration files, the main difference being that *two*
# worker node classes are defined: `small_worker` and `large_worker`
[setup/slurm2]
provider=ansible

master_groups=slurm_master
# there's no actual difference in the configuration used by the two
# node types here -- the only difference is in the node size
# ("flavor", see above)
small_worker_groups=slurm_worker
large_worker_groups=slurm_worker

# install NIS/YP to manage cluster users
global_var_multiuser_cluster=yes


# the `login` section collects information about how to log-in to VMs, including
# SSH key to use for connections
[login/ubuntu]
image_user=ubuntu
image_user_sudo=root
image_sudo=True
# name of SSH key the cloud controller will inject into VMs
user_key_name=elasticluster
# these should match the SSH key named above
user_key_private=~/.ssh/id_rsa
user_key_public=~/.ssh/id_rsa.pub
