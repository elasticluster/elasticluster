#
# Example: deploy a SLURM cluster on Ubuntu 16.04 VMs
#
# This example covers the installation of a SLURM cluster with 4 compute nodes
# and one login/front-end node (which runs SLURM controller daemon, NFS server
# for home directories, and NIS/YP server for accounts). This configuration also
# demoes how to enable add-on services in the cluster:
#
# - the Ganglia monitoring system will be running on every compute node, with
#   metrics collected and graphed on the front-end (open URL
#   http://master001/ganglia)
# - Each compute node is also a GlusterFS "brick" data server and the whole
#   cluster shares a `/glusterfs` file system which stores data across
#   compute nodes.
#
# The configuration is *incomplete* and needs to be complemented with a suitable
# `cloud` section.
#
# For more details about the configuration, see:
# - http://elasticluster.readthedocs.io/en/latest/configure.html
# - http://elasticluster.readthedocs.io/en/latest/playbooks.html#id4
#

[setup/slurm]
provider=ansible

master_groups=slurm_master,ganglia_master,glusterfs_client
worker_groups=slurm_worker,ganglia_monitor,glusterfs_server
# `submit` nodes are normally not used but are available if
# you want to load-balance SSH access to the cluster
submit_groups=slurm_submit,glusterfs_client

# note that Ubuntu 16.04 does *not* install `python2.7` by default, so this
# won't work with official Ubuntu images (still, Ansible requires Python 2)
global_var_ansible_python_interpreter=/usr/bin/python2.7

# install NIS/YP to manage cluster users
global_var_multiuser_cluster=yes


# the `login` section collects information about how to log-in to VMs, including
# SSH key to use for connections
[login/ubuntu]
image_user=ubuntu
image_user_sudo=root
image_sudo=True
# name of SSH key the cloud controller will inject into VMs
user_key_name=elasticluster
# these should match the SSH key named above
user_key_private=~/.ssh/id_rsa
user_key_public=~/.ssh/id_rsa.pub


[cluster/slurm-on-ubuntu16]
setup=slurm
master_nodes=1
worker_nodes=4
ssh_to=master

# this is cloud-specific info (using OpenStack for the example)
cloud=openstack
flavor=1cpu-4ram-hpc
network_ids=c86b320c-9542-4032-a951-c8a068894cc2
security_group=default

# Ubuntu 16.04.1 + python2.7
image_id=bab386b3-2c21-4a67-a146-a658668ac096

# `login` info is -in theory- image-specific
login=ubuntu
