#
# ElastiCluster example: SLURM cluster with GPUs on Google Cloud
#
# This is a complete and self-contained example that should get you
# started with running SLURM clusters with GPU-accelerated compute
# nodes on Google Cloud Engine.  Note that, as of Nov. 2017,
# GPU-enabled VMs are available only in few zones so you may not be
# able to run this example in your default region/zone.
#
# For more details, see:
# - http://googlegenomics.readthedocs.io/en/latest/use_cases/setup_gridengine_cluster_on_compute_engine/index.html#create-a-grid-engine-cluster-on-compute-engine
# - http://elasticluster.readthedocs.io/en/latest/configure.html
# - http://elasticluster.readthedocs.io/en/latest/playbooks.html#id5
#


# SLURM software to be configured by Ansible
#
# (There is nothing Google-specific in the "setup" section; in fact, it can be
# re-used verbatim with *any* cloud provider or base image)
[setup/slurm]
provider=ansible
frontend_groups=slurm_master
# the `cuda` role installs support for NVIDIA GPUs and the CUDA
# runtime and development toolkit
compute_groups=slurm_worker,cuda

# if the NVIDIA drivers are not installed correctly, you might need to
# ElastiCluster to reboot the nodes -- this is accomplished by the
# following variable setting. (ATM, needed on CentOS/RHEL but not on Ubuntu.)
#compute_var_allow_reboot=yes

# If nodes have multiple GPUs, you want to enable cgroup support in
# SLURM to be sure that a job can access only the GPUs that were
# allocated to it.
global_var_slurm_taskplugin=task/cgroup
global_var_slurm_proctracktype=proctrack/cgroup


# Create a cloud provider (call it "google-cloud")
[cloud/google-cloud]
provider=google
gce_project_id=****REPLACE WITH YOUR PROJECT ID****
gce_client_id=****REPLACE WITH YOUR CLIENT ID****
gce_client_secret=****REPLACE WITH YOUR SECRET KEY****


# Create a login (call it "google-login")
#
# In contrast to other cloud providers, GCE creates a personal account on each
# VM so you effectively re-use the same `[login/google]` section across
# different VM images.
[login/google-login]
image_user=****REPLACE WITH YOUR GOOGLE USERID (just the userid, not email)****
image_user_sudo=root
image_sudo=True
user_key_name=elasticluster
# You can generate the keypair with the command: `gcloud compute config-ssh`
user_key_private=~/.ssh/google_compute_engine
user_key_public=~/.ssh/google_compute_engine.pub


# Bring all of the elements together to define a cluster called "slurm-gpu"
[cluster/slurm-gpu]
cloud=google-cloud
login=google-login
setup=slurm
security_group=default
# CHECK WITH OUTPUT FROM: gcloud compute images list
image_id=ubuntu-1604-xenial-v20180105
flavor=n1-standard-1
ssh_to=frontend
frontend_nodes=1
# adjust size of cluster to taste
compute_nodes=4

# now add 2x GPUs (NVidia Tesla K80) to the compute nodes
# note that as of Nov. 2017, GPU-enabled VMs are available only in few zones
[cluster/slurm-gpu/compute]
accelerator_count=2
# use `gcloud compute accelerator-types list` to see what is available
accelerator_type=nvidia-tesla-k80
